XVI

Preface

processes. Many of the hierarchical approaches appeared at the end of the nineties,
and since then a large number of techniques has been introduced. These include new
decompositions of tasks, value functions and policies, and many techniques for au-
tomatically learning task decompositions from interaction with the world. The nal
chapter in this part, Evolutionary Computation for Reinforcement Learning by
Shimon Whiteson surveys evolutionary search for good policy structures (and value
functions). Evolution has always been a good alternative for iterative, incremental
reinforcement learning approaches and both can be used to optimize complex be-
haviors. Evolution is particularly well suited for non-Markov problems and policy
structures for which gradients are unnatural or difcult to compute. In addition, the
chapter surveys evolutionary neural networks for behavior learning.

PROBABILISTIC MODELS OF SELF AND OTHERS

Current articial intelligence has become more and more statistical and probabilis-
tic. Advances in the eld of probabilistic graphical models are used virtually ev-
erywhere, and results for these models  both theoretical as computational  are
effectively used in many sub-elds. This is no different for reinforcement learning.
There are several large sub-elds in which the use of probabilistic models, such as
Bayesian networks, is common practice and the employment of such a universal
set of representations and computational techniques enables fruitful connections to
other research employing similar models.

The rst chapter, Bayesian Reinforcement Learning by Nikos Vlassis, Moham-
mad Ghavamzadeh, Shie Mannor and Pascal Poupart surveys Bayesian techniques
for reinforcement learning. Learning sequential decision making under uncertainty
can be cast in a Bayesian universe where interaction traces provide samples (evi-
dence), and Bayesian inference and learning can be used to nd optimal decision
strategies in a rigorous probabilistic fashion. The next chapter, Partially Observ-
able Markov Decision Processes by Matthijs Spaan surveys representations and
techniques for partially observable problems which are very often cast in a prob-
abilistic framework such as a dynamic Bayesian network, and where probabilis-
tic inference is needed to infer underlying hidden (unobserved) states. The chapter
surveys both model-based as well as model-free methods. Whereas POMDPs are
usually modeled in terms of belief states that capture some form of history (or mem-
ory), a more recent class of methods that focuses on the future is surveyed in the
chapter Predictively Dened Representations of State by David Wingate. These
techniques maintain a belief state used for action selection in terms of probabilistic
predictions about future events. Several techniques are described in which these pre-
dictions are represented compactly and where these are updated based on experience
in the world. So far, most methods focus on the prediction (or; evaluation) problem,
and less on control. The fourth chapter, Game Theory and Multi-agent Reinforce-
ment Learning by Ann Nowe, Peter Vrancx and Yann-Michael De Hauwere moves
to a more general set of problems in which multiple agents learn and interact. It
surveys game-theoretic and multi-agent approaches in reinforcement learning and
shows techniques used to optimize agents in the context of other (learning) agents.
The nal chapter in this part, Decentralized POMDPs by Frans Oliehoek surveys

\x0cPreface

XVII

model-based (dynamic programming) techniques for systems consisting of multiple
agents that have to cooperatively solve a large task that is decomposed into a set
of POMDPs. Such models for example appear in domains where multiple sensors
in different locations together may provide essential information on how to act op-
timally in the world. This chapter builds on methods found in both POMDPs and
multi-agent situations.

DOMAINS AND BACKGROUND

As we have said in the beginning of this preface, reinforcement learning appears as
a method in many other elds of articial intelligence, to optimize behaviors. Thus,
in addition to the many algorithmic advances as described in the previous three parts
of the book, we wanted to include surveys of areas in which reinforcement learning
has been applied successfully. This part features chapters on robotics and games. In
addition, a third chapter reects the growing interest in connecting reinforcement
learning and cognitive neuroscience.

The rst chapter, Psychological and Neuroscientic Connections with Rein-
forcement Learning by Ashvin Shah surveys the connection between reinforcement
learning methods on the one hand and cognition and neuroscience on the other.
Originally many reinforcement learning techniques were derived from insights de-
veloped in psychology by for example Skinner, Thorndike and Watson, and still
much cross-fertilization between psychology and reinforcement learning can hap-
pen. Lately, due to advances in theory about the brain, and especially because testing
and measuring of brain activity (fMRI, EEG, etc.) has become much better, much
research tries to either 1) explain research ndings about the brain in terms of rein-
forcement learning techniques, i.e. which algorithms do really happen in the brain or
2) get inspired by the inner workings of the brain to come up with new algorithms.
The second chapter in this part, Reinforcement Learning in Games by Istvan Szita
surveys the use of reinforcement learning in games. Games is more a general term
here than as used in one of the previous chapters on game theory. Indeed, games
in this chapter amount to board games such as Backgammon and Checkers, but
also computer games such as role-playing and real-time strategy games. Games are
often an exciting test bed for reinforcement learning algorithms (see for example
also Tetris and Mario in the mentioned reinforcement learning competitions), and
in addition to giving many examples, this chapter also tries to outline the main im-
portant aspects involved when applying reinforcement learning in game situations.
The third chapter in this part, Reinforcement Learning in Robotics: a Survey by
Jens Kober and Jan Peters rigorously describes the application of reinforcement
learning to robotics problems. Robotics, because it works with the real, physical
world, features many problems that are challenging for the robust application of re-
inforcement learning. Huge amounts of noisy data, slow training and testing on real
robots, the reality gap between simulators and the real world, and scaling up to high-
dimensional state spaces are just some of the challenging problems discussed here.
Robotics is an exciting area also because of the added possibilities of putting humans
in the loop which can create extra opportunities for imitation learning, learning from
demonstration, and using humans as teachers for robots.

\x0cXVIII

ACKNOWLEDGEMENTS

Preface

Crafting a book such as this can not be done overnight. Many people have put a lot
of work in it to make it happen. First of all, we would like to give a big thanks to all
the authors who have put in all their expertise, time and creativity to write excellent
surveys of their sub-elds. Writing a survey usually takes some extra effort, since
it requires that you know much about a topic, but in addition that you can put all
relevant works in a more general framework. As editors, we are very happy with the
way the authors have accomplished this difcult, yet very useful, task.

A second group of people we would like to thank are the reviewers. They have
provided us with very thorough, and especially very constructive, reviews and these
have made the book even better. We thank these reviewers who agreed to put their
names in the book; thank you very much for all your help: Andrea Bonarini, Prasad
Tadepalli, Sarah Ostentoski, Rich Sutton, Daniel Kudenko, Jesse Hoey, Christopher
Amato, Damien Ernst, Remi Munos, Johannes Fuernkrantz, Juergen Schmidhuber,
Thomas Ruckstiess, Joelle Pineau, Dimitri Bertsekas, John Asmuth, Lisa Torrey,
Yael Niv, Te Thamrongrattanarit, Michael Littman and Csaba Szepesvari.

Thanks also to Rich Sutton who was so kind to write the foreword to this book.
We both consider him as one of the main gures in reinforcement learning, and
in all respects we admire him for all the great contributions he has made to the
eld. He was there in the beginning of modern reinforcement learning, but still he
continuously introduces novel, creative new ways to let agents learn. Thanks Rich!
Editing a book such as this is made much more convenient if you can t it in your
daily scientic life. In that respect, Martijn would like to thank both the Katholieke
Universiteit Leuven (Belgium) as well as the Radboud University Nijmegen (The
Netherlands) for their support. Marco would like to thank the University of Gronin-
gen (The Netherlands) for the same kind of support.

Last but not least, we would like to thank you, the reader, to having picked this
book and having started to read it. We hope it will be useful to you, and hope that
the work you are about to embark on will be incorporated in a subsequent book on
reinforcement learning.

Groningen, Nijmegen,
November 2011

Marco Wiering
Martijn van Otterlo

\x0cContents

Part I Introductory Part

1

2

3

Reinforcement Learning and Markov Decision Processes . . . . . . . . . .
Martijn van Otterlo, Marco Wiering
1.1
1.2
1.3

3
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Learning Sequential Decision Making . . . . . . . . . . . . . . . . . . . . . . . .
5
A Formal Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.3.1 Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . . 10
Policies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.3.2
1.3.3
Optimality Criteria and Discounting . . . . . . . . . . . . . . . . . . 13
Value Functions and Bellman Equations . . . . . . . . . . . . . . . . . . . . . . 15
Solving Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . . . 17
Dynamic Programming: Model-Based Solution Techniques . . . . . . 19
Fundamental DP Algorithms . . . . . . . . . . . . . . . . . . . . . . . . 20
1.6.1
1.6.2
Efcient DP Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
Reinforcement Learning: Model-Free Solution Techniques . . . . . . . 27
1.7.1
Temporal Difference Learning . . . . . . . . . . . . . . . . . . . . . . . 29
1.7.2 Monte Carlo Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
1.7.3
Efcient Exploration and Value Updating . . . . . . . . . . . . . 34
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
1.8
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

1.4
1.5
1.6

1.7

Part II Efcient Solution Frameworks

Batch Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
Sascha Lange, Thomas Gabel, Martin Riedmiller
2.1
2.2

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
The Batch Reinforcement Learning Problem . . . . . . . . . . . . . . . . . . 46
The Batch Learning Problem . . . . . . . . . . . . . . . . . . . . . . . 46
2.2.1
2.2.2
The Growing Batch Learning Problem . . . . . . . . . . . . . . . . 48
Foundations of Batch RL Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 49

2.3

\x0cXX

Contents

2.4

2.5
2.6

Batch RL Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
Kernel-Based Approximate Dynamic Programming . . . . . 53
2.4.1
Fitted Q Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
2.4.2
2.4.3
Least-Squares Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . 57
Identifying Batch Algorithms . . . . . . . . . . . . . . . . . . . . . . . 58
2.4.4
Theory of Batch RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
Batch RL in Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
Neural Fitted Q Iteration (NFQ) . . . . . . . . . . . . . . . . . . . . . 61
2.6.1
NFQ in Control Applications . . . . . . . . . . . . . . . . . . . . . . . . 63
2.6.2
Batch RL for Learning in Multi-agent Systems . . . . . . . . . 65
2.6.3
Deep Fitted Q Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
2.6.4
2.6.5
Applications/ Further References . . . . . . . . . . . . . . . . . . . . 69
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
2.7
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71

Least-Squares Methods for Policy Iteration . . . . . . . . . . . . . . . . . . . . . . 75
Lucian Busoniu, Alessandro Lazaric, Mohammad Ghavamzadeh,
Remi Munos, Robert Babuska, Bart De Schutter
3.1
3.2
3.3

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
Preliminaries: Classical Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . 77
Least-Squares Methods for Approximate Policy Evaluation . . . . . . 79
3.3.1 Main Principles and Taxonomy . . . . . . . . . . . . . . . . . . . . . . 79
3.3.2
The Linear Case and Matrix Form of the Equations . . . . . 81
3.3.3 Model-Free Implementations . . . . . . . . . . . . . . . . . . . . . . . . 85
3.3.4
Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
Online Least-Squares Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . . 89
Example: Car on the Hill . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
Performance Guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
Asymptotic Convergence and Guarantees . . . . . . . . . . . . . 95
3.6.1
Finite-Sample Guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . 98
3.6.2
3.7
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106

3.4
3.5
3.6

Learning and Using Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
Todd Hester, Peter Stone
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
4.2 What Is a Model? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
4.3
4.3.1 Monte Carlo Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
Combining Models and Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
4.4
Sample Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
4.5
Factored Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
4.6
Exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
4.7
Continuous Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
4.8
4.9
Empirical Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
4.10 Scaling Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135

3

4

\x0cContents

XXI

5

6

4.11 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138

Transfer in Reinforcement Learning: A Framework and a Survey . . . 143
Alessandro Lazaric
5.1
5.2

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
A Framework and a Taxonomy for Transfer in Reinforcement
Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
Transfer Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
5.2.1
Taxonomy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
5.2.2

5.3 Methods for Transfer from Source to Target with a Fixed

State-Action Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
5.3.1
Representation Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
5.3.2
Parameter Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
5.3.3

5.4 Methods for Transfer across Tasks with a Fixed

State-Action Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
5.4.1
Instance Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
5.4.2
Representation Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
5.4.3
Parameter Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
5.4.4

5.5 Methods for Transfer from Source to Target Tasks with a

Different State-Action Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
5.5.1
Instance Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
5.5.2
Representation Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
5.5.3
5.5.4
Parameter Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
5.6
Conclusions and Open Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169

Sample Complexity Bounds of Exploration . . . . . . . . . . . . . . . . . . . . . . 175
Lihong Li
6.1
6.2
6.3

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
Formalizing Exploration Efciency . . . . . . . . . . . . . . . . . . . . . . . . . . 178
Sample Complexity of Exploration and PAC-MDP . . . . . . 178
6.3.1
Regret Minimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
6.3.2
6.3.3
Average Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
Bayesian Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
6.3.4
6.4
A Generic PAC-MDP Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
6.5 Model-Based Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
Rmax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
A Generalization of Rmax . . . . . . . . . . . . . . . . . . . . . . . . . . 188
6.6 Model-Free Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
6.7
Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200

6.5.1
6.5.2

\x0c'
